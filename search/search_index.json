{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"all-that-will-matter-is-taste/","title":"All That Will Matter Is Taste","text":"<p>Over the next few years, the way we write code and build software products will shift dramatically. Large Language Models, and by extension agents, are becoming increasingly more capable. Soon enough the barrier to creating whatever interface or application one can imagine won't be technical ability. The only differentiator will be taste.</p> <p>What happens when creating applications isn't a matter of technical ability? And every idea guy has a swarm of developers available at his fingertips? What will differentiate products and software in the marketplace? It's taste. No one wants to read AI written articles, no one wants to look at your AI art. Until we can create models that truely synthesise completely new ideas, the best we have is taste.</p> <p>It will be the responsibility of the builder to interface as closely as possible with their swarm of agents to guide the product. The product owner will steer the ship of agents in the way he wants using his own inspiration and taste. The inspired builder will be able to effortlessly realise his vision for whatever he decides to craft. What will make his product unique is the cumulation of signal that he has been able to pass through to his swarm of agents. No spending hours wrestling with React - just taste realised instantly.</p>"},{"location":"all-that-will-matter-is-taste/#sources","title":"Sources","text":"<p>John Schulman OpenAI on Dwarkesh Podcast - 'I think in 1 or 2 years - you could imagine having the models carry out a whole coding project'</p> <p>Large Multimodal Model Development Improvement from OpenAI (GPT-4o) and Meta (Chameleon)</p> <p>Transformers don't do good</p> <p>BBL Drizzy sample created through trial and error on Udio</p>"},{"location":"autogen-with-groq/","title":"Affordable Agents \u2014 Autogen with Llama3\u201370b on Groq","text":""},{"location":"autogen-with-groq/#why-use-groq","title":"Why Use Groq","text":"<p>Running GPT-4 on Groq is expensive, especially if you want to create a user-facing application that will need to handle a lot of requests.</p>"},{"location":"autogen-with-groq/#openai-vs-groq-cost-comparison","title":"OpenAi vs Groq Cost Comparison","text":"<p>Cost per million tokens (input/output):</p> <p></p> <p>While Llama-3\u201370B is not as performant as GPT-4, the affordable pricing offered by Groq adds viability to many use cases, including Autogen experimentation.</p>"},{"location":"autogen-with-groq/#using-groq-with-autogen","title":"Using Groq with Autogen","text":"<p>Groq offers 'OpenAI Compatibility'. By mimicking the functionality of the OpenAI API, we can easily substitute Groq into our existing applications that are built on OpenAI without having to worry about changing schemas and API outputs.</p> <p>Fortunately, this makes using Groq with Autogen very easy:</p> <ol> <li>Create an API key for Groq</li> <li>Set up the Autogen LLM config with Groq:</li> </ol> <pre><code>llm_config = {\n    \"model\": \"llama3-70b-8192\",\n    \"api_key\": os.environ[\"GROQ_API_KEY\"],\n    \"base_url\": \"https://api.groq.com/openai/v1/\"\n}\n</code></pre> <ol> <li>Run your agents with Groq!</li> </ol> <pre><code>import autogen\nfrom autogen import AssistantAgent, UserProxyAgent\nimport os\n\nllm_config = {\n    \"model\": \"llama3-70b-8192\",\n    \"api_key\": os.environ[\"GROQ_API_KEY\"],\n    \"base_url\": \"https://api.groq.com/openai/v1/\"\n}\n\nwith autogen.coding.DockerCommandLineCodeExecutor(work_dir=\"coding\") as code_executor:\n    assistant = AssistantAgent(\"assistant\", llm_config=llm_config)\n    user_proxy = UserProxyAgent(\n        \"user_proxy\", code_execution_config={\"executor\": code_executor}\n    )\n\n    # Start the chat\n    user_proxy.initiate_chat(\n        assistant,\n        message=\"What date is today? Which big tech stock has the largest year-to-date gain this year? How much is the gain?\"\n    )\n</code></pre>"}]}